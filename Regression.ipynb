{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ba580ee-5915-48c8-a773-90b775d2f623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an\\nexample of each.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an\n",
    "example of each.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d9babcf-c404-435c-adbf-f92ca14ef9e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"### **Simple Linear Regression:**\\n- **Definition:** Models the relationship between a single independent variable and a dependent variable using a straight line.\\n- **Equation:** \\\\( y = \\x08eta_0 + \\x08eta_1x + \\\\epsilon \\\\)\\n  - \\\\( y \\\\) = dependent variable\\n  - \\\\( x \\\\) = independent variable\\n  - \\\\( \\x08eta_0 \\\\) = intercept\\n  - \\\\( \\x08eta_1 \\\\) = slope coefficient\\n  - \\\\( \\\\epsilon \\\\) = error term\\n- **Example:** Predicting a student's exam score based on the number of hours studied.\\n\\n### **Multiple Linear Regression:**\\n- **Definition:** Models the relationship between multiple independent variables and a dependent variable using a linear equation.\\n- **Equation:** \\\\( y = \\x08eta_0 + \\x08eta_1x_1 + \\x08eta_2x_2 + \\\\ldots + \\x08eta_nx_n + \\\\epsilon \\\\)\\n  - \\\\( y \\\\) = dependent variable\\n  - \\\\( x_1, x_2, \\\\ldots, x_n \\\\) = independent variables\\n  - \\\\( \\x08eta_0 \\\\) = intercept\\n  - \\\\( \\x08eta_1, \\x08eta_2, \\\\ldots, \\x08eta_n \\\\) = coefficients for independent variables\\n  - \\\\( \\\\epsilon \\\\) = error term\\n- **Example:** Predicting a house price based on multiple factors such as size, number of bedrooms, and location.\\n\\n**Summary:**\\n- **Simple Linear Regression** involves one independent variable.\\n- **Multiple Linear Regression** involves multiple independent variables.\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''### **Simple Linear Regression:**\n",
    "- **Definition:** Models the relationship between a single independent variable and a dependent variable using a straight line.\n",
    "- **Equation:** \\( y = \\beta_0 + \\beta_1x + \\epsilon \\)\n",
    "  - \\( y \\) = dependent variable\n",
    "  - \\( x \\) = independent variable\n",
    "  - \\( \\beta_0 \\) = intercept\n",
    "  - \\( \\beta_1 \\) = slope coefficient\n",
    "  - \\( \\epsilon \\) = error term\n",
    "- **Example:** Predicting a student's exam score based on the number of hours studied.\n",
    "\n",
    "### **Multiple Linear Regression:**\n",
    "- **Definition:** Models the relationship between multiple independent variables and a dependent variable using a linear equation.\n",
    "- **Equation:** \\( y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\ldots + \\beta_nx_n + \\epsilon \\)\n",
    "  - \\( y \\) = dependent variable\n",
    "  - \\( x_1, x_2, \\ldots, x_n \\) = independent variables\n",
    "  - \\( \\beta_0 \\) = intercept\n",
    "  - \\( \\beta_1, \\beta_2, \\ldots, \\beta_n \\) = coefficients for independent variables\n",
    "  - \\( \\epsilon \\) = error term\n",
    "- **Example:** Predicting a house price based on multiple factors such as size, number of bedrooms, and location.\n",
    "\n",
    "**Summary:**\n",
    "- **Simple Linear Regression** involves one independent variable.\n",
    "- **Multiple Linear Regression** involves multiple independent variables.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63b492ae-54b8-4e7c-a097-a8bb7e9686f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in\\na given dataset?'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in\n",
    "a given dataset?'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd618ae9-02a3-4b51-a934-88c69d35afb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### **Assumptions of Linear Regression:**\\n\\n1. **Linearity:**\\n   - **Assumption:** The relationship between the independent and dependent variables is linear.\\n   - **Check:** Use scatter plots to visualize if the relationship between predictors and the response is approximately linear.\\n\\n2. **Independence:**\\n   - **Assumption:** Observations are independent of each other.\\n   - **Check:** Plot residuals versus time or sequence to check for patterns indicating autocorrelation.\\n\\n3. **Homoscedasticity:**\\n   - **Assumption:** The residuals (errors) have constant variance.\\n   - **Check:** Plot residuals versus fitted values. The spread of residuals should be consistent across all levels of the fitted values.\\n\\n4. **Normality of Residuals:**\\n   - **Assumption:** The residuals are normally distributed.\\n   - **Check:** Use a Q-Q plot or histogram of residuals. The points should follow a straight line or the histogram should resemble a normal distribution.\\n\\n5. **No Multicollinearity:**\\n   - **Assumption:** Independent variables are not highly correlated with each other.\\n   - **Check:** Calculate the Variance Inflation Factor (VIF) for each predictor. High VIF values (typically VIF > 10) indicate multicollinearity.\\n\\n### **Summary:**\\n- **Linearity**: Check with scatter plots.\\n- **Independence**: Check residual plots for autocorrelation.\\n- **Homoscedasticity**: Check with residuals versus fitted values plot.\\n- **Normality**: Check with Q-Q plot or histogram of residuals.\\n- **No Multicollinearity**: Check VIF values for predictors.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''### **Assumptions of Linear Regression:**\n",
    "\n",
    "1. **Linearity:**\n",
    "   - **Assumption:** The relationship between the independent and dependent variables is linear.\n",
    "   - **Check:** Use scatter plots to visualize if the relationship between predictors and the response is approximately linear.\n",
    "\n",
    "2. **Independence:**\n",
    "   - **Assumption:** Observations are independent of each other.\n",
    "   - **Check:** Plot residuals versus time or sequence to check for patterns indicating autocorrelation.\n",
    "\n",
    "3. **Homoscedasticity:**\n",
    "   - **Assumption:** The residuals (errors) have constant variance.\n",
    "   - **Check:** Plot residuals versus fitted values. The spread of residuals should be consistent across all levels of the fitted values.\n",
    "\n",
    "4. **Normality of Residuals:**\n",
    "   - **Assumption:** The residuals are normally distributed.\n",
    "   - **Check:** Use a Q-Q plot or histogram of residuals. The points should follow a straight line or the histogram should resemble a normal distribution.\n",
    "\n",
    "5. **No Multicollinearity:**\n",
    "   - **Assumption:** Independent variables are not highly correlated with each other.\n",
    "   - **Check:** Calculate the Variance Inflation Factor (VIF) for each predictor. High VIF values (typically VIF > 10) indicate multicollinearity.\n",
    "\n",
    "### **Summary:**\n",
    "- **Linearity**: Check with scatter plots.\n",
    "- **Independence**: Check residual plots for autocorrelation.\n",
    "- **Homoscedasticity**: Check with residuals versus fitted values plot.\n",
    "- **Normality**: Check with Q-Q plot or histogram of residuals.\n",
    "- **No Multicollinearity**: Check VIF values for predictors.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfe29164-3708-490b-a248-5808641a874f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using\\na real-world scenario.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using\n",
    "a real-world scenario.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38b10b14-1e19-42bd-811b-2420c6c5d1c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### **Interpreting Slope and Intercept in Linear Regression:**\\n\\n1. **Intercept (\\\\(\\x08eta_0\\\\)):**\\n   - **Definition:** The value of the dependent variable when all independent variables are zero.\\n   - **Interpretation:** Represents the baseline level of the dependent variable when predictors have no effect.\\n   - **Example:** In predicting house prices, the intercept represents the baseline price when all features (e.g., size, number of bedrooms) are zero.\\n\\n2. **Slope (\\\\(\\x08eta_1\\\\)):**\\n   - **Definition:** The change in the dependent variable for a one-unit change in the independent variable.\\n   - **Interpretation:** Indicates the strength and direction of the relationship between the independent and dependent variables.\\n   - **Example:** In predicting house prices, if the slope for size is $200 per square foot, this means that for each additional square foot of house size, the price increases by $200.\\n\\n### **Example Scenario:**\\n\\n**Predicting Exam Scores:**\\n- **Model:** \\\\( \\text{Score} = \\x08eta_0 + \\x08eta_1 \\times \\text{Hours Studied} \\\\)\\n  - **Intercept (\\\\(\\x08eta_0\\\\)):** Suppose it is 50. This means a student who studies 0 hours is expected to score 50.\\n  - **Slope (\\\\(\\x08eta_1\\\\)):** Suppose it is 5. This means each additional hour of study is associated with an increase of 5 points in the exam score.\\n\\n**Summary:**\\n- **Intercept** gives the baseline value of the dependent variable.\\n- **Slope** shows the change in the dependent variable per unit change in the independent variable.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''### **Interpreting Slope and Intercept in Linear Regression:**\n",
    "\n",
    "1. **Intercept (\\(\\beta_0\\)):**\n",
    "   - **Definition:** The value of the dependent variable when all independent variables are zero.\n",
    "   - **Interpretation:** Represents the baseline level of the dependent variable when predictors have no effect.\n",
    "   - **Example:** In predicting house prices, the intercept represents the baseline price when all features (e.g., size, number of bedrooms) are zero.\n",
    "\n",
    "2. **Slope (\\(\\beta_1\\)):**\n",
    "   - **Definition:** The change in the dependent variable for a one-unit change in the independent variable.\n",
    "   - **Interpretation:** Indicates the strength and direction of the relationship between the independent and dependent variables.\n",
    "   - **Example:** In predicting house prices, if the slope for size is $200 per square foot, this means that for each additional square foot of house size, the price increases by $200.\n",
    "\n",
    "### **Example Scenario:**\n",
    "\n",
    "**Predicting Exam Scores:**\n",
    "- **Model:** \\( \\text{Score} = \\beta_0 + \\beta_1 \\times \\text{Hours Studied} \\)\n",
    "  - **Intercept (\\(\\beta_0\\)):** Suppose it is 50. This means a student who studies 0 hours is expected to score 50.\n",
    "  - **Slope (\\(\\beta_1\\)):** Suppose it is 5. This means each additional hour of study is associated with an increase of 5 points in the exam score.\n",
    "\n",
    "**Summary:**\n",
    "- **Intercept** gives the baseline value of the dependent variable.\n",
    "- **Slope** shows the change in the dependent variable per unit change in the independent variable.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38e7055a-ccb2-45f7-978a-41fcd32b6280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q4. Explain the concept of gradient descent. How is it used in machine learning?'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Q4. Explain the concept of gradient descent. How is it used in machine learning?'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44a9c4a2-af19-46a3-8548-360408af6fdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### **Concept of Gradient Descent:**\\n\\n- **Definition:** Gradient descent is an optimization algorithm used to minimize the loss function of a machine learning model by iteratively updating model parameters.\\n- **Objective:** To find the parameter values that minimize the error between predicted and actual values.\\n\\n### **How It Works:**\\n\\n1. **Initialize Parameters:** Start with initial guesses for the model parameters.\\n2. **Compute Gradient:** Calculate the gradient of the loss function with respect to each parameter. The gradient indicates the direction and rate of the steepest ascent.\\n3. **Update Parameters:** Adjust the parameters in the opposite direction of the gradient by a certain step size (learning rate). This step reduces the loss.\\n4. **Repeat:** Iterate the process until the parameters converge to values that minimize the loss function.\\n\\n### **Usage in Machine Learning:**\\n\\n- **Training Models:** Gradient descent is used to train various machine learning models, such as linear regression, logistic regression, and neural networks, by optimizing their loss functions.\\n- **Variants:** Different variants like Stochastic Gradient Descent (SGD) and Mini-batch Gradient Descent are used depending on the size of the dataset and computational efficiency.\\n\\n### **Summary:**\\n\\nGradient descent helps in finding optimal parameters for a model by iteratively reducing the loss function, thus improving model performance.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''### **Concept of Gradient Descent:**\n",
    "\n",
    "- **Definition:** Gradient descent is an optimization algorithm used to minimize the loss function of a machine learning model by iteratively updating model parameters.\n",
    "- **Objective:** To find the parameter values that minimize the error between predicted and actual values.\n",
    "\n",
    "### **How It Works:**\n",
    "\n",
    "1. **Initialize Parameters:** Start with initial guesses for the model parameters.\n",
    "2. **Compute Gradient:** Calculate the gradient of the loss function with respect to each parameter. The gradient indicates the direction and rate of the steepest ascent.\n",
    "3. **Update Parameters:** Adjust the parameters in the opposite direction of the gradient by a certain step size (learning rate). This step reduces the loss.\n",
    "4. **Repeat:** Iterate the process until the parameters converge to values that minimize the loss function.\n",
    "\n",
    "### **Usage in Machine Learning:**\n",
    "\n",
    "- **Training Models:** Gradient descent is used to train various machine learning models, such as linear regression, logistic regression, and neural networks, by optimizing their loss functions.\n",
    "- **Variants:** Different variants like Stochastic Gradient Descent (SGD) and Mini-batch Gradient Descent are used depending on the size of the dataset and computational efficiency.\n",
    "\n",
    "### **Summary:**\n",
    "\n",
    "Gradient descent helps in finding optimal parameters for a model by iteratively reducing the loss function, thus improving model performance.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b0684c7-aa6a-4aee-941a-e553a1d54624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43530896-c149-48c0-82db-d023d2c8610c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### **Multiple Linear Regression Model:**\\n\\n- **Definition:** A statistical technique that models the relationship between a dependent variable and two or more independent variables using a linear equation.\\n- **Equation:** \\\\( y = \\x08eta_0 + \\x08eta_1x_1 + \\x08eta_2x_2 + \\\\ldots + \\x08eta_nx_n + \\\\epsilon \\\\)\\n  - \\\\( y \\\\) = dependent variable\\n  - \\\\( x_1, x_2, \\\\ldots, x_n \\\\) = independent variables\\n  - \\\\( \\x08eta_0 \\\\) = intercept\\n  - \\\\( \\x08eta_1, \\x08eta_2, \\\\ldots, \\x08eta_n \\\\) = coefficients for each independent variable\\n  - \\\\( \\\\epsilon \\\\) = error term\\n\\n### **Difference from Simple Linear Regression:**\\n\\n- **Number of Independent Variables:**\\n  - **Simple Linear Regression:** Models the relationship between one independent variable and the dependent variable.\\n  - **Multiple Linear Regression:** Models the relationship between multiple independent variables and the dependent variable.\\n\\n- **Equation Complexity:**\\n  - **Simple Linear Regression:** \\\\( y = \\x08eta_0 + \\x08eta_1x + \\\\epsilon \\\\)\\n  - **Multiple Linear Regression:** Includes multiple terms for each independent variable, making the model more complex.\\n\\n### **Summary:**\\n\\n- **Simple Linear Regression** involves a single predictor, while **Multiple Linear Regression** involves multiple predictors, allowing for a more comprehensive model of relationships and interactions between variables.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''### **Multiple Linear Regression Model:**\n",
    "\n",
    "- **Definition:** A statistical technique that models the relationship between a dependent variable and two or more independent variables using a linear equation.\n",
    "- **Equation:** \\( y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\ldots + \\beta_nx_n + \\epsilon \\)\n",
    "  - \\( y \\) = dependent variable\n",
    "  - \\( x_1, x_2, \\ldots, x_n \\) = independent variables\n",
    "  - \\( \\beta_0 \\) = intercept\n",
    "  - \\( \\beta_1, \\beta_2, \\ldots, \\beta_n \\) = coefficients for each independent variable\n",
    "  - \\( \\epsilon \\) = error term\n",
    "\n",
    "### **Difference from Simple Linear Regression:**\n",
    "\n",
    "- **Number of Independent Variables:**\n",
    "  - **Simple Linear Regression:** Models the relationship between one independent variable and the dependent variable.\n",
    "  - **Multiple Linear Regression:** Models the relationship between multiple independent variables and the dependent variable.\n",
    "\n",
    "- **Equation Complexity:**\n",
    "  - **Simple Linear Regression:** \\( y = \\beta_0 + \\beta_1x + \\epsilon \\)\n",
    "  - **Multiple Linear Regression:** Includes multiple terms for each independent variable, making the model more complex.\n",
    "\n",
    "### **Summary:**\n",
    "\n",
    "- **Simple Linear Regression** involves a single predictor, while **Multiple Linear Regression** involves multiple predictors, allowing for a more comprehensive model of relationships and interactions between variables.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "675b9e21-71b4-4bbd-a5e6-b17b874e84a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and\\naddress this issue?'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and\n",
    "address this issue?'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c2171a6-2e40-420a-a752-d235734f095f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### **Concept of Multicollinearity:**\\n\\n- **Definition:** Multicollinearity occurs when two or more independent variables in a multiple linear regression model are highly correlated, leading to redundancy and potential instability in the coefficient estimates.\\n- **Impact:** It can cause inflated standard errors for the coefficients, making it difficult to determine the individual effect of each predictor and reducing the interpretability of the model.\\n\\n### **Detection:**\\n\\n1. **Correlation Matrix:** Check pairwise correlations between independent variables. High correlations (e.g., > 0.8) suggest potential multicollinearity.\\n   \\n2. **Variance Inflation Factor (VIF):** Calculate VIF for each predictor. A VIF value greater than 10 indicates high multicollinearity.\\n   \\n3. **Condition Index:** Analyze the condition number of the predictor matrix. A high condition number (e.g., > 30) suggests multicollinearity.\\n\\n### **Addressing Multicollinearity:**\\n\\n1. **Remove Variables:** Exclude one of the highly correlated predictors from the model.\\n   \\n2. **Combine Variables:** Create composite variables or use techniques like Principal Component Analysis (PCA) to combine correlated predictors.\\n   \\n3. **Regularization:** Apply regularization methods such as Ridge Regression (L2 regularization) which can handle multicollinearity by adding a penalty to the size of coefficients.\\n\\n### **Summary:**\\n\\nMulticollinearity can distort regression results. It can be detected using correlation matrices, VIF, and condition index, and addressed by removing variables, combining them, or applying regularization techniques.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''### **Concept of Multicollinearity:**\n",
    "\n",
    "- **Definition:** Multicollinearity occurs when two or more independent variables in a multiple linear regression model are highly correlated, leading to redundancy and potential instability in the coefficient estimates.\n",
    "- **Impact:** It can cause inflated standard errors for the coefficients, making it difficult to determine the individual effect of each predictor and reducing the interpretability of the model.\n",
    "\n",
    "### **Detection:**\n",
    "\n",
    "1. **Correlation Matrix:** Check pairwise correlations between independent variables. High correlations (e.g., > 0.8) suggest potential multicollinearity.\n",
    "   \n",
    "2. **Variance Inflation Factor (VIF):** Calculate VIF for each predictor. A VIF value greater than 10 indicates high multicollinearity.\n",
    "   \n",
    "3. **Condition Index:** Analyze the condition number of the predictor matrix. A high condition number (e.g., > 30) suggests multicollinearity.\n",
    "\n",
    "### **Addressing Multicollinearity:**\n",
    "\n",
    "1. **Remove Variables:** Exclude one of the highly correlated predictors from the model.\n",
    "   \n",
    "2. **Combine Variables:** Create composite variables or use techniques like Principal Component Analysis (PCA) to combine correlated predictors.\n",
    "   \n",
    "3. **Regularization:** Apply regularization methods such as Ridge Regression (L2 regularization) which can handle multicollinearity by adding a penalty to the size of coefficients.\n",
    "\n",
    "### **Summary:**\n",
    "\n",
    "Multicollinearity can distort regression results. It can be detected using correlation matrices, VIF, and condition index, and addressed by removing variables, combining them, or applying regularization techniques.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2356e633-66a3-4367-a601-958c64f5f05f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q7. Describe the polynomial regression model. How is it different from linear regression?'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Q7. Describe the polynomial regression model. How is it different from linear regression?'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a3154cf-f1b2-4972-a342-d384ae73f108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### **Polynomial Regression Model:**\\n\\n- **Definition:** Polynomial regression extends linear regression by modeling the relationship between the independent and dependent variables as an \\\\( n \\\\)-th degree polynomial.\\n- **Equation:** \\\\( y = \\x08eta_0 + \\x08eta_1x + \\x08eta_2x^2 + \\\\ldots + \\x08eta_nx^n + \\\\epsilon \\\\)\\n  - \\\\( y \\\\) = dependent variable\\n  - \\\\( x \\\\) = independent variable\\n  - \\\\( \\x08eta_0, \\x08eta_1, \\\\ldots, \\x08eta_n \\\\) = coefficients\\n  - \\\\( \\\\epsilon \\\\) = error term\\n\\n### **Difference from Linear Regression:**\\n\\n- **Model Form:**\\n  - **Linear Regression:** Models a straight-line relationship (first-degree polynomial): \\\\( y = \\x08eta_0 + \\x08eta_1x + \\\\epsilon \\\\).\\n  - **Polynomial Regression:** Models a curved relationship using higher-degree polynomials, allowing for more flexibility in capturing non-linear patterns.\\n\\n- **Flexibility:**\\n  - **Linear Regression:** Limited to linear relationships.\\n  - **Polynomial Regression:** Can capture non-linear relationships by including polynomial terms.\\n\\n### **Summary:**\\n\\nPolynomial regression introduces polynomial terms to capture non-linear relationships, whereas linear regression only models linear relationships. This added flexibility in polynomial regression helps in fitting more complex data patterns.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''### **Polynomial Regression Model:**\n",
    "\n",
    "- **Definition:** Polynomial regression extends linear regression by modeling the relationship between the independent and dependent variables as an \\( n \\)-th degree polynomial.\n",
    "- **Equation:** \\( y = \\beta_0 + \\beta_1x + \\beta_2x^2 + \\ldots + \\beta_nx^n + \\epsilon \\)\n",
    "  - \\( y \\) = dependent variable\n",
    "  - \\( x \\) = independent variable\n",
    "  - \\( \\beta_0, \\beta_1, \\ldots, \\beta_n \\) = coefficients\n",
    "  - \\( \\epsilon \\) = error term\n",
    "\n",
    "### **Difference from Linear Regression:**\n",
    "\n",
    "- **Model Form:**\n",
    "  - **Linear Regression:** Models a straight-line relationship (first-degree polynomial): \\( y = \\beta_0 + \\beta_1x + \\epsilon \\).\n",
    "  - **Polynomial Regression:** Models a curved relationship using higher-degree polynomials, allowing for more flexibility in capturing non-linear patterns.\n",
    "\n",
    "- **Flexibility:**\n",
    "  - **Linear Regression:** Limited to linear relationships.\n",
    "  - **Polynomial Regression:** Can capture non-linear relationships by including polynomial terms.\n",
    "\n",
    "### **Summary:**\n",
    "\n",
    "Polynomial regression introduces polynomial terms to capture non-linear relationships, whereas linear regression only models linear relationships. This added flexibility in polynomial regression helps in fitting more complex data patterns.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a2aff79-9a77-493f-9f8f-92e5617c677d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q8. What are the advantages and disadvantages of polynomial regression compared to linear\\nregression? In what situations would you prefer to use polynomial regression?'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Q8. What are the advantages and disadvantages of polynomial regression compared to linear\n",
    "regression? In what situations would you prefer to use polynomial regression?'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dbfa800e-4d3d-4491-a4c4-3dc1ea54ea1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"### **Advantages of Polynomial Regression:**\\n\\n1. **Captures Non-Linearity:** Can model complex, non-linear relationships between variables, providing a better fit for data with curves or bends.\\n2. **Flexibility:** By adjusting the degree of the polynomial, you can fit a wide range of data patterns.\\n\\n### **Disadvantages of Polynomial Regression:**\\n\\n1. **Overfitting:** Higher-degree polynomials can lead to overfitting, where the model fits the training data too closely and performs poorly on unseen data.\\n2. **Complexity:** Polynomial regression models become increasingly complex with higher degrees, making them harder to interpret and computationally intensive.\\n3. **Extrapolation Issues:** Polynomial models can produce unrealistic predictions outside the range of the training data due to their curvature.\\n\\n### **When to Use Polynomial Regression:**\\n\\n- **Complex Relationships:** Use when the relationship between independent and dependent variables is not linear and displays curvature.\\n- **Improving Fit:** When linear regression does not adequately fit the data, and polynomial terms can improve model performance.\\n- **Data Patterns:** Use when domain knowledge suggests a non-linear relationship between predictors and response.\\n\\n### **Summary:**\\n\\nPolynomial regression offers flexibility for capturing non-linear relationships but risks overfitting and increased complexity. It's preferred when data shows clear non-linear patterns and a linear model fails to capture the relationship adequately.\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''### **Advantages of Polynomial Regression:**\n",
    "\n",
    "1. **Captures Non-Linearity:** Can model complex, non-linear relationships between variables, providing a better fit for data with curves or bends.\n",
    "2. **Flexibility:** By adjusting the degree of the polynomial, you can fit a wide range of data patterns.\n",
    "\n",
    "### **Disadvantages of Polynomial Regression:**\n",
    "\n",
    "1. **Overfitting:** Higher-degree polynomials can lead to overfitting, where the model fits the training data too closely and performs poorly on unseen data.\n",
    "2. **Complexity:** Polynomial regression models become increasingly complex with higher degrees, making them harder to interpret and computationally intensive.\n",
    "3. **Extrapolation Issues:** Polynomial models can produce unrealistic predictions outside the range of the training data due to their curvature.\n",
    "\n",
    "### **When to Use Polynomial Regression:**\n",
    "\n",
    "- **Complex Relationships:** Use when the relationship between independent and dependent variables is not linear and displays curvature.\n",
    "- **Improving Fit:** When linear regression does not adequately fit the data, and polynomial terms can improve model performance.\n",
    "- **Data Patterns:** Use when domain knowledge suggests a non-linear relationship between predictors and response.\n",
    "\n",
    "### **Summary:**\n",
    "\n",
    "Polynomial regression offers flexibility for capturing non-linear relationships but risks overfitting and increased complexity. It's preferred when data shows clear non-linear patterns and a linear model fails to capture the relationship adequately.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569b1398-d626-4c82-bd40-20eed0974ab8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
